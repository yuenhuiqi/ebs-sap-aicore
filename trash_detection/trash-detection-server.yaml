apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: trash-detection-smu-team02 # executable id, must be unique across all your workflows (YAML files)
  annotations:
    scenarios.ai.sap.com/description: "Trash detection using Transformers"
    scenarios.ai.sap.com/name: "trash-detection-smu-team02" # Scenario name should be the use case
    executables.ai.sap.com/description: "Image PyTorch GPU Serving executable"
    executables.ai.sap.com/name: "trash-server-smu-team02" # Executable name should describe the workflow in the use case
    artifacts.ai.sap.com/trash.kind: "model" # Helps in suggesting the kind of artifact that can be generated.
  labels:
    scenarios.ai.sap.com/id: "image-detection"
    ai.sap.com/version: "1.0"
spec:
  template:
      apiVersion: "serving.kserve.io/v1beta1"
      metadata:
        annotations: |
          autoscaling.knative.dev/metric: concurrency   # condition when to scale
          autoscaling.knative.dev/target: 1
          autoscaling.knative.dev/targetBurstCapacity: 0
        labels: |
          ai.sap.com/resourcePlan: starter # computing power
      spec: |
        predictor:
          imagePullSecrets:
            - name: ebs-t2-aicore   # your docker registry secret
          minReplicas: 1
          maxReplicas: 5    # how much to scale
          containers:
          - name: kserve-container
            image: "docker.io/yuenhuiqi/trash:01"
            ports:
              - containerPort: 9001    # customizable port
                protocol: TCP
            command: ["/bin/sh", "-c"]
            args:
              - >
                set -e && echo "Starting" && gunicorn --chdir /app/src main:app -b 0.0.0.0:9001 # filename `main` flask variable `app`
  imagePullSecrets:
    - name: ebs-t2-trash # your docker registry secret
  entrypoint: mypipeline
  templates:
  - name: mypipeline
    steps:
    - - name: mypredictor
        template: mycodeblock1
  - name: mycodeblock1
    # inputs:
    #   artifacts:  # placeholder for cloud storage attachements
    #     - name: apple-training # a name for the placeholder
    #       path: /app/data/archive/fruits-360_dataset/fruits-360/Training/Apple # where to copy in the Dataset in the Docker image
    #     - name: lemon-training
    #       path: /app/data/archive/fruits-360_dataset/fruits-360/Training/Lemon
    #     - name: apple-testing
    #       path: /app/data/archive/fruits-360_dataset/fruits-360/Test/Apple
    #     - name: lemon-testing
    #       path: /app/data/archive/fruits-360_dataset/fruits-360/Test/Lemon
    # outputs:
    #   artifacts:
    #     - name: trashmodel # name of the artifact generated, and folder name when placed in S3, complete directory will be `../<executaion_id>/housepricemodel`
    #       globalName: trashmodel # local identifier name to the workflow, also used above in annotation
    #       path: /app/model/ # from which folder in docker image (after running workflow step) copy contents to cloud storage
    #       archive:
    #         none:   # specify not to compress while uploading to cloud
    #           {}
          
    container:
      image: docker.io/yuenhuiqi/trash:01 # Your docker image name
      command: ["/bin/sh", "-c"]
      args:
        - "python /app/src/main.py"